defaults:
  - default
  - _self_

# ===================
# ====== basic ======
# ===================
parallel: "single" # "single" or "host" or "worker"
sync_mode: "sync"  # "async", "sync"
save_path: '/home/<usrname>/logs/host'
aggregated_save_path: '/home/<usrname>/logs/aggregate'
synthetic: False

# ===================
# ====== path =======
# ===================
android_avd_home: '/home/<usrname>/.android/avd'
emulator_path: '/home/<usrname>/.android/emulator/emulator'
adb_path: '/home/<usrname>/.android/platform-tools/adb'
cache_dir: '/home/<usrname>/.cache'
assets_path: '/home/<usrname>/DistRL/assets/task_set'

# ===================
# ====== model ======
# ===================
policy_lm: '/home/<usrname>/AutoUI/Auto-UI-Base'
critic_lm: 'roberta-base'

# ===================
# ====== train ======
# ===================
train_iterations: 400
train_time: 100 # (in minutes) -1 for unlimited
save_freq: 1

capacity: 10000 # replay buffer size
batch_size: 2 # replay buffer sample batch size
warmup_iter: 0 # how many iterations to only collect data and evaluate before training

# start_checkpoint_path: '/home/distrl/models/ckpts/general-off2on-digirl'
collect_num: 128

# offline_data_path: null
offline_data_path: '/home/<usrname>/data/distrl_data/warmup_trajectories_penalty.pt'
offline_actor_iterations: 30
offline_critic_iterations: 20
offline_trajectory_critic_iterations: 20

# ===================
# ===== collect =====
# ===================
collect_iterations: 1
bsize: 8 # number of emulators parallelly on the machine
rollout_size: 8 # how many trajectories to collect between training iterations (should be multiple of bsize)
avd_name: 'android_30'
save_images: False
